import json
import time
from typing import List, Optional, Dict, Any

from fastapi import FastAPI, Request, Response
from schemas.request import PredictionRequest
from utils.logger import setup_logger
from services.search import search
from services.llm import generate_answer
from services.answer_detector import has_answer_options
from config import settings

app = FastAPI()
logger = None


@app.on_event("startup")
async def startup_event():
    global logger
    logger = await setup_logger()

    if not settings.MISTRAL_API_KEY:
        await logger.error("Mistral API key not configured!")

    if not settings.TAVILY_API_KEY:
        await logger.error("Tavily API key not configured!")


@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    body = await request.body()

    await logger.info(
        f"Incoming request: {request.method} {request.url}\n"
        f"Request body: {body.decode()}"
    )

    try:
        response = await call_next(request)
    except Exception as e:
        response = Response(
            content=json.dumps({
                "id": -1,
                "answer": None,
                "reasoning": f"Internal server error: {str(e)}",
                "sources": []
            }),
            status_code=500,
            media_type="application/json"
        )

    process_time = time.time() - start_time
    response_body = b""

    async for chunk in response.body_iterator:
        response_body += chunk

    await logger.info(
        f"Request completed: {request.method} {request.url}\n"
        f"Status: {response.status_code}\n"
        f"Duration: {process_time:.3f}s"
    )

    return Response(
        content=response_body,
        status_code=response.status_code,
        headers=dict(response.headers),
        media_type=response.media_type,
    )


@app.post("/api/request")
async def predict(body: PredictionRequest):
    response_data = {
        "id": body.id,
        "answer": None,
        "reasoning": "",
        "sources": []
    }

    try:
        # Проверка наличия вариантов ответа
        has_options = has_answer_options(body.query)

        # Поиск информации
        search_results = search(body.query) if settings.TAVILY_API_KEY else []

        # Формирование контекста
        context = "\n".join(
            [f"Источник {i+1}: {res['content'][:500]}"
             for i, res in enumerate(search_results[:3])]
        ) if search_results else "Информация не найдена"

        # Генерация ответа
        if settings.MISTRAL_API_KEY:
            llm_response = await generate_answer(body.query, context)

            # Установка answer в None если нет вариантов
            response_data["answer"] = int(llm_response["answer"]) if (
                has_options and
                llm_response.get("answer") and
                str(llm_response["answer"]).isdigit()
            ) else None

            response_data["reasoning"] = f"{llm_response.get('reasoning', '')} (Generated by Mistral AI)"[
                :1000]
        else:
            response_data["reasoning"] = "Mistral API not configured"[:1000]

        # Формирование источников
        response_data["sources"] = [res["url"]
                                    for res in search_results[:3]
                                    if isinstance(res.get("url"), str)]

    except Exception as e:
        error_msg = f"Error processing request: {str(e)}"
        await logger.error(f"Request {body.id} failed: {error_msg}")
        response_data["reasoning"] = error_msg[:1000]

    return Response(
        content=json.dumps(response_data, ensure_ascii=False),
        media_type="application/json; charset=utf-8",
        status_code=200
    )
